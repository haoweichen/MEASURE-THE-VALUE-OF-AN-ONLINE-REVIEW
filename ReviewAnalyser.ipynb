{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############## document information ##############\n",
      "\n",
      "total_nb_words:\n",
      "1467\n",
      "\n",
      "############## label CNN performance is as follows: ##############\n",
      "\n",
      "mean_squared_error:\n",
      "0.028211111177\n",
      "\n",
      "############## sentiment CNN performance is as follows: ##############\n",
      "\n",
      "mean_squared_error:\n",
      "0.0575333320777\n",
      "label prediction:\n",
      "{'the burger is good': {'service': 0.015223131515085697, 'food': 0.10371792316436768, 'price': 0.040295105427503586, 'environment': 0.03286609798669815, 'amenities': 0.04846002906560898, 'location': 0.031283553689718246}, 'location location location': {'service': 0.03556470572948456, 'food': 0.02766220085322857, 'price': 0.05505352467298508, 'environment': 0.043794240802526474, 'amenities': 0.04809872806072235, 'location': 0.075876384973526}, 'the staff is nice': {'service': 0.12029092758893967, 'food': 0.047340329736471176, 'price': 0.07041794061660767, 'environment': 0.11408496648073196, 'amenities': 0.1301555186510086, 'location': 0.0875726044178009}}\n",
      "sentiment prediction:\n",
      "{'the steak is juicy and crespy': 0.8327191472053528, 'I love this restaurant': 0.45797914266586304}\n",
      "\n",
      "############## quality performance is as follows: ##############\n",
      "\n",
      "{'label_predict': {'the beef steak is crispy and juicy.': {'service': 0.02323237434029579, 'food': 0.5098682641983032, 'price': 0.03779054433107376, 'environment': 0.01927579753100872, 'amenities': 0.048344921320676804, 'location': 0.03025202639400959}, 'The ice cream is sweet': {'service': 0.04818231612443924, 'food': 0.11522509157657623, 'price': 0.0817394033074379, 'environment': 0.09252684563398361, 'amenities': 0.13600750267505646, 'location': 0.0798456147313118}, 'the beef steak is good.': {'service': 0.004668662324547768, 'food': 0.36365029215812683, 'price': 0.02323226071894169, 'environment': 0.009591289795935154, 'amenities': 0.0271315798163414, 'location': 0.013763482682406902}, 'the beer is good too.': {'service': 0.004804430529475212, 'food': 0.15010198950767517, 'price': 0.026674069464206696, 'environment': 0.01525488868355751, 'amenities': 0.03125057369470596, 'location': 0.017991583794355392}}, 'sentiment_predict': {'the beef steak is crispy and juicy.': 0.7116113305091858, 'The ice cream is sweet': 0.6937782764434814, 'the beef steak is good.': 0.5937855243682861, 'the beer is good too.': 0.8809815049171448}, 'review_predict': {'the beef steak is crispy and juicy. The ice cream is sweet': 0.42996241997382034, 'the beef steak is good. the beer is good too.': 0.23963263325543255}}\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import json\n",
    "import gensim\n",
    "import nltk,string\n",
    "from random import shuffle\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from nltk import tokenize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import matplotlib  \n",
    "matplotlib.use('Agg') \n",
    "from matplotlib.pyplot import plot,savefig \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "\n",
    "DOCVECTOR_MODEL=\"docvector_model\"\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "BEST_LABEL_WEIGHT_FILEPATH=\"best_label_weight\"\n",
    "BEST_SENT_MODEL_FILEPATH=\"best_sent_model\"\n",
    "BEST_SENT_WEIGHT_FILEPATH=\"best_sent_weight\"\n",
    "QUALITY_MODEL=\"quality_model\"\n",
    "MAX_NB_WORDS=99\n",
    "MAX_DOC_LEN=200\n",
    "EMBEDDING_DIM=200\n",
    "FILTER_SIZES=[2,3,4]\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 40\n",
    "LABELS = ['amenities','environment','food','location','price','service','sentiment']\n",
    "\n",
    "class ReviewAnalyser(object):\n",
    "    \n",
    "    # review's ann model\n",
    "    ann_model = None\n",
    "    # label's cnn model\n",
    "    label_model = None\n",
    "    # labels input padding sequence\n",
    "    label_padding_sequence = None\n",
    "    # labels actual classification\n",
    "    label_act = None\n",
    "    # labels test set feature\n",
    "    label_X_test = None\n",
    "    # labels test set labels\n",
    "    label_Y_set = None\n",
    "    # labels validation set feature\n",
    "    label_X_train = None\n",
    "    # labels validation set labels\n",
    "    label_Y_train = None\n",
    "    # sentiment's cnn model\n",
    "    sent_model = None\n",
    "    # sentiment input padding sequence\n",
    "    sent_padding_sequence = None\n",
    "    # sentiment actual classification\n",
    "    sent_act = None\n",
    "    # labels test set\n",
    "    sent_test_set = None\n",
    "    # sentiment's validation set\n",
    "    sent_validation_set = None\n",
    "    # sentitment's test set feature\n",
    "    sent_X_test = None\n",
    "    # sentitment's test set labels\n",
    "    sent_Y_set = None\n",
    "    # sentitment's validation set feature\n",
    "    sent_X_train = None\n",
    "    # sentitment's validation set labels\n",
    "    sent_Y_train = None\n",
    "    # doc2vector's cnn model\n",
    "    wv_model = None\n",
    "    \n",
    "    def __init__(self, data): \n",
    "        self.data = data;\n",
    "        \n",
    "    @staticmethod    \n",
    "    def cnn_model(FILTER_SIZES, \\\n",
    "        # filter sizes as a list\n",
    "        MAX_NB_WORDS, \\\n",
    "        # total number of words\n",
    "        MAX_DOC_LEN, \\\n",
    "        # max words in a doc\n",
    "        NUM_OUTPUT_UNITS=1, \\\n",
    "        # number of output units\n",
    "        EMBEDDING_DIM=200, \\\n",
    "        # word vector dimension\n",
    "        NUM_FILTERS=64, \\\n",
    "        # number of filters for all size\n",
    "        DROP_OUT=0.5, \\\n",
    "        # dropout rate\n",
    "        PRETRAINED_WORD_VECTOR=None,\\\n",
    "        # Whether to use pretrained word vectors\n",
    "        LAM=0.01,\\\n",
    "        ACTIVATION='sigmoid'):            \n",
    "        # regularization coefficient\n",
    "    \n",
    "        main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                           dtype='int32', name='main_input')\n",
    "\n",
    "        if PRETRAINED_WORD_VECTOR is not None:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                            trainable=False,\\\n",
    "                            name='embedding')(main_input)\n",
    "        else:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            name='embedding')(main_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        for f in FILTER_SIZES:\n",
    "            conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                          activation='relu', name='conv_'+str(f))(embed_1)\n",
    "            conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "            conv = Flatten(name='flat_'+str(f))(conv)\n",
    "            conv_blocks.append(conv)\n",
    "\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "        drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "        dense = Dense(192, activation='relu',\\\n",
    "                        kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "        preds = Dense(NUM_OUTPUT_UNITS, activation=ACTIVATION, name='output')(dense)\n",
    "        model = Model(inputs=main_input, outputs=preds)\n",
    "\n",
    "#         model.compile(loss=\"binary_crossentropy\", \\\n",
    "#                   optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        \n",
    "        model.compile(loss=\"mean_squared_error\", \\\n",
    "                  optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    # training to change document into vector using gensim\n",
    "    def pretrain(self, RETRAIN=0):\n",
    "        with open(\"word_sample.json\", 'r') as f:\n",
    "            reviews=[]\n",
    "            for line in f: \n",
    "                review = json.loads(line) \n",
    "                try:\n",
    "                    review[\"text\"].strip().lower().encode('ascII')\n",
    "                except:\n",
    "                    # do nothing\n",
    "                    a = 1\n",
    "                else:\n",
    "                    reviews.append(review[\"text\"])\n",
    "\n",
    "        sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "                     for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                         if token not in string.punctuation and \\\n",
    "                         len(token.strip(string.punctuation).strip())>=2]\\\n",
    "                     for doc in reviews]\n",
    "\n",
    "        docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "        \n",
    "        if RETRAIN==0 and os.path.exists(DOCVECTOR_MODEL):\n",
    "            self.wv_model = doc2vec.Doc2Vec.load(DOCVECTOR_MODEL)\n",
    "#             print self.wv_model\n",
    "        else:\n",
    "            self.wv_model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "            self.wv_model.build_vocab(docs)\n",
    "            for epoch in range(30):\n",
    "                # shuffle the documents in each epoch\n",
    "                shuffle(docs)\n",
    "                # in each epoch, all samples are used\n",
    "                self.wv_model.train(docs, total_examples=len(docs), epochs=1)\n",
    "                \n",
    "            self.wv_model.save(DOCVECTOR_MODEL)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price'\")\n",
    "#         print self.wv_model.wv.most_similar('price', topn=5)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price' but not relevant to 'bathroom'\")\n",
    "#         print self.wv_model.wv.most_similar(positive=['price','money'], negative=['bathroom'], topn=5)\n",
    "\n",
    "#         print(\"Similarity between 'price' and 'bathroom':\")\n",
    "#         print self.wv_model.wv.similarity('price','bathroom') \n",
    "\n",
    "#         print(\"Similarity between 'price' and 'charge':\")\n",
    "#         print self.wv_model.wv.similarity('price','charge') \n",
    "\n",
    "#         print self.wv_model.wv\n",
    "\n",
    "    # training labels CNN\n",
    "    def trainLebels(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        # fetch labels for each sentence        \n",
    "        for subdata in self.data[2][0:500]:\n",
    "            label = []\n",
    "            for d in subdata.split(\",\"):\n",
    "                label.append(float(d.strip()))\n",
    "            labels.append(label)\n",
    "        \n",
    "        Y = np.copy(labels)\n",
    "        self.label_act = Y\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.label_padding_sequence = padded_sequences\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                        padded_sequences[0:500], Y[0:500], test_size=0.3, random_state=0)\n",
    "        \n",
    "        self.label_X_train = X_train\n",
    "        self.label_Y_train = Y_train\n",
    "        self.label_X_test = X_test\n",
    "        self.label_Y_test = Y_test\n",
    "        \n",
    "        if(RETRAIN == 0 and os.path.exists(BEST_MODEL_FILEPATH)):\n",
    "#                 self.label_model.load_weights(BEST_MODEL_FILEPATH)\n",
    "                self.label_model = load_model(BEST_MODEL_FILEPATH)\n",
    "#                 pred=self.label_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "        \n",
    "        self.label_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                        MAX_DOC_LEN, NUM_OUTPUT_UNITS=6, \\\n",
    "                        PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_LABEL_WEIGHT_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "        \n",
    "        training=self.label_model.fit(X_train, Y_train, \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[X_test, Y_test], verbose=2)\n",
    "        \n",
    "        self.label_model.save(BEST_MODEL_FILEPATH)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    # training sentiment CNN        \n",
    "    def trainSentiment(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        for i,subdata in enumerate(self.data[3][0:500]):\n",
    "            labels.append([subdata])\n",
    "\n",
    "        Y_labels = np.copy(labels)\n",
    "        Y = Y_labels\n",
    "        self.sent_act = Y\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.sent_padding_sequence = padded_sequences\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences[0:500], Y[0:500], test_size=0.3, random_state=0)\n",
    "        self.sent_X_train = X_train\n",
    "        self.sent_X_test = X_test\n",
    "        self.sent_Y_train = Y_train\n",
    "        self.sent_Y_test = Y_test\n",
    "        \n",
    "        if(RETRAIN == 0 and os.path.exists(BEST_SENT_MODEL_FILEPATH)):\n",
    "                self.sent_model = load_model(BEST_SENT_MODEL_FILEPATH)\n",
    "                pred=self.sent_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "        \n",
    "        \n",
    "        self.sent_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                    MAX_DOC_LEN, \\\n",
    "                    PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_SENT_WEIGHT_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "        training=self.sent_model.fit(X_train, Y_train, \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[X_test, Y_test], verbose=2) \n",
    "        \n",
    "        self.sent_model.save(BEST_SENT_MODEL_FILEPATH)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def checkLabelPerform(self):\n",
    "        predicted=self.label_model.predict(self.label_X_test)\n",
    "        predicted = np.round(predicted, decimals=1)\n",
    "        Y_actual = self.label_Y_test\n",
    "        return mean_squared_error(Y_actual, predicted)\n",
    "        \n",
    "\n",
    "    def checkSentimentPerform(self):\n",
    "        pred=self.sent_model.predict(self.sent_X_test)\n",
    "        predicted=np.reshape(pred, -1)\n",
    "        predicted = np.round(predicted, decimals=1)\n",
    "        Y_actual = self.sent_Y_test\n",
    "        return mean_squared_error(Y_actual, predicted)\n",
    "        \n",
    "       \n",
    "    # check document information to determine the value of hyper-parameter\n",
    "    def checkDocInform(self):  \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        total_nb_words=len(tokenizer.word_counts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        print \"\\n############## document information ##############\\n\"\n",
    "        print \"total_nb_words:\"\n",
    "        print(total_nb_words)\n",
    "\n",
    "        word_counts=pd.DataFrame(\\\n",
    "                    tokenizer.word_counts.items(), \\\n",
    "                    columns=['word','count'])\n",
    "        df=word_counts['count'].value_counts().reset_index()\n",
    "        df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "        plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "        plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "        plt.xlabel('Word Frequency')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('word_freq.jpg')\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "        \n",
    "        sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "        df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "        df.columns=['index','counts']\n",
    "\n",
    "        df=df.sort_values(by='index')\n",
    "        df['percent']=df['counts']/len(sen_len)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "        \n",
    "        plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "        plt.xlabel('Sentence Length')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('sent_len.jpg')\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "        return\n",
    "        \n",
    "    # predict labels for text, need to execute trainLabels first\n",
    "    def predictLabels(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.label_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            dict1 = {}\n",
    "            pred_list = sub_pred[i].tolist()\n",
    "            for i, sub_pred_list in enumerate(pred_list):\n",
    "                dict1[LABELS[i]] = pred_list[i]\n",
    "            rtn[key] = dict1\n",
    "        return rtn\n",
    "        \n",
    "    # predict sentiments for text, need to execute trainSentiment first    \n",
    "    def predictSentiment(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.sent_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            rtn[key] = sub_pred[i].tolist()[0]\n",
    "        return rtn\n",
    "    \n",
    "    # predict quality for reviews, need to execute trainLabels,trainSentiment and trainQuality first    \n",
    "    def predictQuality(self, review_arr=[]):\n",
    "        text_arr=[]\n",
    "        sentence_review_mapping = []\n",
    "        data = []\n",
    "        rows = {}\n",
    "        if len(review_arr)==0:\n",
    "            return\n",
    "        for i, rev in enumerate(review_arr):\n",
    "            rev_sent = tokenize.sent_tokenize(rev)\n",
    "            for sent in rev_sent:\n",
    "                text_arr.append(sent)\n",
    "                sentence_review_mapping.append((i,sent))\n",
    "            \n",
    "        label_predict = self.predictLabels(text_arr)\n",
    "        sentiment_predict = self.predictSentiment(text_arr)\n",
    "    \n",
    "        for mapping in sentence_review_mapping:\n",
    "            rows[mapping[1]] = {}\n",
    "            rows[mapping[1]][\"review_id\"] = mapping[0]\n",
    "            rows[mapping[1]][\"sentence\"] = mapping[1]\n",
    "            tmp = label_predict[mapping[1]]\n",
    "            rows[mapping[1]][\"labels\"] = str(tmp[\"amenities\"])+','+str(tmp[\"environment\"])+','+str(tmp[\"food\"])+','+str(tmp[\"location\"])+','+str(tmp[\"price\"])+','+str(tmp[\"service\"])\n",
    "            rows[mapping[1]][\"sentiment\"] = sentiment_predict[mapping[1]]\n",
    "        \n",
    "        data = []\n",
    "        for key in rows:\n",
    "            subdata=[]\n",
    "            subdata.append(rows[key][\"review_id\"])\n",
    "            subdata.append(rows[key][\"sentence\"])\n",
    "            subdata.append(rows[key][\"labels\"])\n",
    "            subdata.append(rows[key][\"sentiment\"])\n",
    "            data.append(subdata)\n",
    "        df=pd.DataFrame(data, columns=[\"review_id\",\"sentence\",\"labels\",\"sentiment\"])\n",
    "        res = self.gradeReview(df.values.tolist())\n",
    "        \n",
    "        predicted = {}\n",
    "        for k in res:\n",
    "            res[k]['quality'] = res[k]['quality']/res[k]['items']\n",
    "            predicted[review_arr[k]] = res[k]['quality']\n",
    "        \n",
    "        rtn = {\n",
    "            \"label_predict\": label_predict,\n",
    "            \"sentiment_predict\": sentiment_predict,\n",
    "            \"review_predict\": predicted\n",
    "        }\n",
    "        return rtn\n",
    "\n",
    "    # for analysing the data sample\n",
    "    def dataSamplePlt(self):\n",
    "        x = np.arange(0, 500, 1);\n",
    "        y = np.copy(self.data[3][0:500])\n",
    "        plt.xlabel('data sample items')\n",
    "        plt.ylabel('sentiment')\n",
    "        plt.plot(x, y,'ro',label=\"the level of objectivity\")\n",
    "        plt.legend(loc='lower right')\n",
    "        savefig('data_sample_sentiment.jpg')\n",
    "        plt.close('all')\n",
    "        \n",
    "        amenities = []\n",
    "        environment = []\n",
    "        food = []\n",
    "        location = []\n",
    "        price = []\n",
    "        service = []\n",
    "        for subdata in self.data[2][0:500]:\n",
    "            label = []\n",
    "            for key,value in enumerate(subdata.split(\",\")):\n",
    "                if key == 0:\n",
    "                    amenities.append(float(value.strip()))\n",
    "                if key == 1:\n",
    "                    environment.append(float(value.strip()))\n",
    "                if key == 2:\n",
    "                    food.append(float(value.strip()))\n",
    "                if key == 3:\n",
    "                    location.append(float(value.strip()))\n",
    "                if key == 4:\n",
    "                    price.append(float(value.strip()))\n",
    "                if key == 5:\n",
    "                    service.append(float(value.strip()))\n",
    "        plt.xlabel('data sample items')\n",
    "        plt.ylabel('labels')\n",
    "        plt.scatter(x,np.copy(amenities),color='red',label=\"amenities\")\n",
    "        plt.scatter(x,np.copy(environment),color='green',label=\"environment\")\n",
    "        plt.scatter(x,np.copy(food),color='blue',label=\"food\")\n",
    "        plt.scatter(x,np.copy(location),color='yellow',label=\"location\")\n",
    "        plt.scatter(x,np.copy(price),color='black',label=\"price\")\n",
    "        plt.scatter(x,np.copy(service),color='orange',label=\"service\")\n",
    "        plt.legend(loc='lower right')\n",
    "        savefig('data_sample_labels.jpg')\n",
    "        plt.close('all')\n",
    "        return \n",
    "    \n",
    "    def gradeCSV(self):\n",
    "        rows = self.gradeReview(self.data[0:500].values.tolist())\n",
    "        for k in rows:\n",
    "            rows[k]['quality'] = rows[k]['quality']/rows[k]['items']\n",
    "            self.data.loc[self.data[0] == k, 4] = rows[k]['quality']\n",
    "        print self.data.head(10)\n",
    "        self.data.to_csv('data_sample3.csv')\n",
    "    \n",
    "    \n",
    "    def gradeReview(self, reviews):\n",
    "        rows = {}\n",
    "        for subdata in reviews:\n",
    "            sent = subdata[3]\n",
    "            labels = subdata[2].split(',')\n",
    "            if rows.has_key(subdata[0]):\n",
    "                rows[subdata[0]][\"items\"] = float(rows[subdata[0]][\"items\"]) + 1\n",
    "                for key,label in enumerate(labels):\n",
    "                    rows[subdata[0]]['quality'] = rows[subdata[0]]['quality'] + (float(label)*float(sent))\n",
    "            else:\n",
    "                rows[subdata[0]] = {\n",
    "                    'items': 1.0,\n",
    "                    'quality': 0.0\n",
    "                }\n",
    "                for key,label in enumerate(labels):\n",
    "                    rows[subdata[0]]['quality'] = rows[subdata[0]]['quality'] + (float(label)*float(sent))\n",
    "                \n",
    "        return rows\n",
    "            \n",
    "# main function here\n",
    "data=pd.read_csv(\"data_sample2.csv\",header=None)\n",
    "ra = ReviewAnalyser(data)\n",
    "ra.pretrain(RETRAIN=0)\n",
    "ra.checkDocInform()\n",
    "ra.dataSamplePlt()\n",
    "ra.trainLebels(RETRAIN=0)\n",
    "print \"\\n############## label CNN performance is as follows: ##############\\n\"\n",
    "print(\"mean_squared_error:\")\n",
    "print ra.checkLabelPerform()\n",
    "ra.trainSentiment(RETRAIN=0)\n",
    "print \"\\n############## sentiment CNN performance is as follows: ##############\\n\"\n",
    "print(\"mean_squared_error:\")\n",
    "print ra.checkSentimentPerform()\n",
    "print \"label prediction:\"\n",
    "label_predict = ra.predictLabels(text_arr=[\"the burger is good\", \"the staff is nice\",\"location location location\"])\n",
    "print label_predict\n",
    "print \"sentiment prediction:\"\n",
    "sentiment_predict = ra.predictSentiment(text_arr=[\"the steak is juicy and crespy\", \"I love this restaurant\"])\n",
    "print sentiment_predict\n",
    "print \"\\n############## quality performance is as follows: ##############\\n\"\n",
    "print ra.predictQuality(review_arr=[\"the beef steak is good. the beer is good too.\", \"the beef steak is crispy and juicy. The ice cream is sweet\"])\n",
    "# print ra.predictQuality(review_arr=[\"the beef steak is crispy and juicy. the beer is cold\"])\n",
    "# print ra.predictQuality(review_arr=[\"It is excellent.It is excellent.It is excellent.\"])\n",
    "# ra.gradeCSV()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
