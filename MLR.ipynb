{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      "0.0376666666667\n",
      "sentiment:\n",
      "0.0675333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import json\n",
    "import nltk,string\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "MAX_NB_WORDS=99\n",
    "MAX_DOC_LEN=200\n",
    "EMBEDDING_DIM=200\n",
    "DOCVECTOR_MODEL=\"docvector_model\"\n",
    "\n",
    "class MLR(object):\n",
    "    \n",
    "    def __init__(self, data): \n",
    "        self.data = data;\n",
    "    \n",
    "    def pretrain(self, RETRAIN=0):\n",
    "        with open(\"word_sample.json\", 'r') as f:\n",
    "            reviews=[]\n",
    "            for line in f: \n",
    "                review = json.loads(line) \n",
    "                try:\n",
    "                    review[\"text\"].strip().lower().encode('ascII')\n",
    "                except:\n",
    "                    # do nothing\n",
    "                    a = 1\n",
    "                else:\n",
    "                    reviews.append(review[\"text\"])\n",
    "\n",
    "        sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "                     for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                         if token not in string.punctuation and \\\n",
    "                         len(token.strip(string.punctuation).strip())>=2]\\\n",
    "                     for doc in reviews]\n",
    "\n",
    "        docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "        \n",
    "        if RETRAIN==0 and os.path.exists(DOCVECTOR_MODEL):\n",
    "            self.wv_model = doc2vec.Doc2Vec.load(DOCVECTOR_MODEL)\n",
    "#             print self.wv_model\n",
    "        else:\n",
    "            self.wv_model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "            self.wv_model.build_vocab(docs)\n",
    "            for epoch in range(30):\n",
    "                # shuffle the documents in each epoch\n",
    "                shuffle(docs)\n",
    "                # in each epoch, all samples are used\n",
    "                self.wv_model.train(docs, total_examples=len(docs), epochs=1)\n",
    "                \n",
    "            self.wv_model.save(DOCVECTOR_MODEL)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price'\")\n",
    "#         print self.wv_model.wv.most_similar('price', topn=5)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price' but not relevant to 'bathroom'\")\n",
    "#         print self.wv_model.wv.most_similar(positive=['price','money'], negative=['bathroom'], topn=5)\n",
    "\n",
    "#         print(\"Similarity between 'price' and 'bathroom':\")\n",
    "#         print self.wv_model.wv.similarity('price','bathroom') \n",
    "\n",
    "#         print(\"Similarity between 'price' and 'charge':\")\n",
    "#         print self.wv_model.wv.similarity('price','charge') \n",
    "\n",
    "#         print self.wv_model.wv\n",
    "\n",
    "    def checkLabelsPerform(self):\n",
    "        labels = []\n",
    "        # fetch labels for each sentence        \n",
    "        for subdata in self.data[2][0:500]:\n",
    "            label = []\n",
    "            for d in subdata.split(\",\"):\n",
    "                label.append(float(d.strip()))\n",
    "            labels.append(label)\n",
    "\n",
    "        Y = np.copy(labels)\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        \n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.label_padding_sequence = padded_sequences\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                        padded_sequences[0:500], Y[0:500], test_size=0.3, random_state=0)\n",
    "        \n",
    "        clf = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        predicted = clf.predict(X_test)\n",
    "        predicted = np.round(predicted, decimals=1)\n",
    "        Y_actual = Y_test\n",
    "        return mean_squared_error(Y_actual, predicted)\n",
    "\n",
    "    def checkSentimentPerform(self):\n",
    "        labels = []\n",
    "        for i,subdata in enumerate(self.data[3][0:500]):\n",
    "            labels.append([subdata])\n",
    "\n",
    "        Y_labels = np.copy(labels)\n",
    "        Y = Y_labels\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.sent_padding_sequence = padded_sequences\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences[0:500], Y[0:500], test_size=0.3, random_state=0)\n",
    "        \n",
    "        clf = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        predicted = clf.predict(X_test)\n",
    "        predicted = np.round(predicted, decimals=1)\n",
    "        Y_actual = Y_test\n",
    "        return mean_squared_error(Y_actual, predicted)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "df = pd.read_csv(\"data_sample2.csv\",header=None)\n",
    "df.head(10)\n",
    "mlr = MLR(df)\n",
    "mlr.pretrain()\n",
    "print \"labels:\"\n",
    "mlr.checkLabelsPerform()\n",
    "print \"sentiment:\"\n",
    "mlr.checkSentimentPerform()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
